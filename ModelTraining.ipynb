{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1c95dc-20a2-44c1-ae3a-ac3da0e2dbf6",
   "metadata": {},
   "source": [
    "# Network Training \n",
    "\n",
    "For the actual training, I ran '$ python RLManager.py', and getting a model to train in the notebook would take considerable rewriting of many of the functions we used to train the model, so this notebook is more of a quicker way to show the training process \n",
    "\n",
    "Many of the functions and classes are defined and imported from RLManager.py, and I did my best comment when this is true to avoid confusion when reading the code. The important functions are: \n",
    "* optimze_model()\n",
    "\n",
    "If you want to run an interactive mode, call '$ python Interactive.py'  \n",
    "* To use a model that you have trained, rename the directory 'Output' to 'GameModel' in your file explorer  \n",
    "* You may want to rename the current GameModel directory or move it to somewhere you can reuse it for later\n",
    "\n",
    "In order to run, you will need to have\n",
    "* torch\n",
    "* gymnasium\n",
    "* numpy\n",
    "* pygame\n",
    "* matplotlib\n",
    "\n",
    "This script also need access to the functions and classes defined in RLManager.py and MissileEnv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296efad4-b1f5-41fa-87ff-f0519ee7d1d9",
   "metadata": {},
   "source": [
    "# Github Access \n",
    "\n",
    "In order to simplify running the code, you can pull from the Github I set up for the project: https://github.com/matty-cua/MissileTesting\n",
    "\n",
    "This includes requirements.txt for quicker install with pip, and also has the final trained model\n",
    "\n",
    "There are also other exploratory notebooks that I used, such as PathTesting.ipynb that I used to develop the random paths used for the target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0eb4435-27e9-4202-8b48-4a338f5bd592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom imports...\n",
      "Finished all imports\n"
     ]
    }
   ],
   "source": [
    "print(\"Custom imports...\")\n",
    "from MissileEnv import MissileEnv\n",
    "from RLManager import * \n",
    "\n",
    "# Imports \n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from collections import namedtuple, deque \n",
    "import random \n",
    "from pathlib import Path \n",
    "import numpy as np\n",
    "\n",
    "# Torch imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Finished all imports\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c1a0ad-ce2e-4990-96ba-4042e647574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputs \n",
    "num_episodes = 100\n",
    "save_loc = Path('Output')\n",
    "\n",
    "# Hyperparameters \n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99        \n",
    "EPS_START = 0.8\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 50000\n",
    "TAU = 0.05  # Started at 0.005\n",
    "LR = 1e-4\n",
    "SAVE_EVERY = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c21bbe7-13b4-4c6b-93ca-db264b99e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment \n",
    "env = MissileEnv()\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "# n_observations = len(state)\n",
    "n_observations = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5941e235-987a-4eb7-8c63-e51e2a6c16a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up policies...\n",
      "Initialize optimizer and replay memory...\n"
     ]
    }
   ],
   "source": [
    "# Prep for training \n",
    "\n",
    "# Ensure the output directory exists (will crash if not) \n",
    "save_loc.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set up policies \n",
    "print(\"Setting up policies...\")\n",
    "model_class = DQN  # Use dense model (opposed to DQN_RNN model (does not train w/ current setup)) \n",
    "policy_net = model_class(n_observations, n_actions).to(device)\n",
    "target_net = model_class(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Set up the optimizer and memory (store output from env for training) \n",
    "print(\"Initialize optimizer and replay memory...\")\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# training step counter + episode duration counter\n",
    "save_step = 0\n",
    "point_step = 0\n",
    "save_points = np.zeros(2*SAVE_EVERY)\n",
    "check_ref = 0\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "average_rewards = []\n",
    "epsilons = []\n",
    "target_hit = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c99aa-f368-4170-8e95-f1c3ad92dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning training loop...\")\n",
    "for i_episode in range(num_episodes):\n",
    "    print(f\"Episode: {i_episode + 1}\")\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    reward_total = 0\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Update some stats \n",
    "        reward_total += reward \n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "            print(\"    - Hit the Target!\")\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            average_rewards.append(reward_total.cpu().numpy()[0])\n",
    "            cep = eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "                math.exp(-1. * steps_done / EPS_DECAY)\n",
    "            epsilons.append(cep)\n",
    "            target_hit.append(terminated)\n",
    "            # plot_durations()\n",
    "            break\n",
    "\n",
    "    # Try and print out rewards \n",
    "    print(f\"    - Duration: {episode_durations[-1]}\")\n",
    "    print(f\"    - Reward  : {average_rewards[-1]}\")\n",
    "    print(f\"    - Eps     : {cep}\")\n",
    "\n",
    "    # Checkpointing \n",
    "    if point_step >= len(save_points): \n",
    "        point_step = 0\n",
    "    save_points[point_step] = terminated\n",
    "    point_step += 1\n",
    "    if save_step >= SAVE_EVERY: \n",
    "        save_step = 0\n",
    "        if np.mean(point_step) > check_ref: \n",
    "            check_ref = np.mean(point_step)\n",
    "            torch.save(target_net_state_dict, save_loc / 'CheckpointWeights.wts')\n",
    "            print(\"======== MODEL CHECKPOINT =======\")\n",
    "    else: \n",
    "        save_step += 1\n",
    "            \n",
    "\n",
    "    # Intermediate plots\n",
    "    if (i_episode + 1) % 50 == 0: \n",
    "        if use_plots: \n",
    "            try: \n",
    "                # Plot and save (can't display it) \n",
    "                episode_plot(episode_durations, 'episode')\n",
    "                plt.gcf().savefig(save_loc / 'DurationPlot.png')\n",
    "                episode_plot(average_rewards, 'reward')\n",
    "                plt.gcf().savefig(save_loc / 'RewardPlot.png')\n",
    "                episode_plot(target_hit, 'target hit')\n",
    "                plt.gcf().savefig(save_loc / 'TargetHits.png')\n",
    "                episode_plot(epsilons, 'epsilon')\n",
    "                plt.gcf().savefig(save_loc / 'EpsilonPlot.png')\n",
    "            except Exception as e: \n",
    "                print('ERROR: Could not show plots: ') \n",
    "                print(e)\n",
    "        # Intermediate stats \n",
    "        torch.save(\n",
    "            {\n",
    "                'durations': episode_durations, \n",
    "                'rewards': average_rewards, \n",
    "                'impacts': target_hit, \n",
    "                'epsilon': epsilons \n",
    "            }, \n",
    "            save_loc / 'ModelStats', \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148756aa-0e96-413b-963b-64ccb43fa2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model + stats \n",
    "torch.save(target_net_state_dict, save_loc / 'ModelWeights.wts')\n",
    "torch.save(target_net, save_loc / 'ModelTorch.pkl')\n",
    "\n",
    "# Plot and save training stats \n",
    "if use_plots: \n",
    "    episode_plot(episode_durations, 'episode')\n",
    "    plt.gcf().savefig(save_loc / 'DurationPlot.png')\n",
    "    episode_plot(average_rewards, 'reward')\n",
    "    plt.gcf().savefig(save_loc / 'RewardPlot.png')\n",
    "    episode_plot(target_hit, 'target hit')\n",
    "    plt.gcf().savefig(save_loc / 'TargetHits.png')\n",
    "    episode_plot(epsilons, 'epsilon')\n",
    "    plt.gcf().savefig(save_loc / 'EpsilonPlot.png')\n",
    "\n",
    "# Save the training stats (just episode duration for now, should figure out better loss plots like reward and such)\n",
    "torch.save(\n",
    "    {\n",
    "        'durations': episode_durations, \n",
    "        'rewards': average_rewards, \n",
    "        'impacts': target_hit, \n",
    "        'epsilon': epsilons \n",
    "    }, \n",
    "    save_loc / 'ModelStats', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a99bb6-b2d2-45e5-b09d-1725fb1b175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots \n",
    "def movmean(data, Nmean=50): \n",
    "    data = target_impact_lengths\n",
    "    flat = np.ones(target_impact_lengths.shape)\n",
    "    avg_filt = np.ones(Nmean) / Nmean\n",
    "    edge_destroyer = 1 / np.convolve(flat, avg_filt, 'same')\n",
    "    out = np.convolve(data, avg_filt, 'same') * edge_destroyer\n",
    "    return out \n",
    "\n",
    "# Reload the script (for debugging and such) \n",
    "reload(RLManager)\n",
    "\n",
    "# mod_path = Path('GameModel')  # The model to be used in interactive simulations  \n",
    "mod_path = Path('Output')  # The last trained model \n",
    "\n",
    "# Load the data dictionary (torch.load() is just a wrapper for pickle.load()) \n",
    "data = mod_path / 'ModelStats'\n",
    "dd = torch.load(data, weights_only=False)\n",
    "epp = lambda s: RLManager.episode_plot(dd[s], s)  # quick wrapper for quicker coding \n",
    "\n",
    "# Get the episode length for each target impact \n",
    "target_impact_lengths = np.array(dd['durations'])[dd['impacts']]\n",
    "\n",
    "epp('impacts')\n",
    "plt.show()\n",
    "epp('rewards')\n",
    "plt.show()\n",
    "epp('durations')\n",
    "plt.show()\n",
    "epp('epsilon')\n",
    "plt.show()\n",
    "\n",
    "# Plot episode length of each target impact \n",
    "RLManager.episode_plot(target_impact_lengths, 'impact_durations')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
